{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Generation_LSTM_V2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#One to Many - Multi Layer - LSTM Model (Text Generation)"
      ],
      "metadata": {
        "id": "E8MILotU4oSo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CqNHuQ0WbjTF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Signal Class to handle keybord interrupts\n",
        "import signal\n",
        "\n",
        "class DelayedKeyboardInterrupt(object):\n",
        "    def __enter__(self):\n",
        "        self.signal_received = False\n",
        "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
        "\n",
        "    def handler(self, sig, frame):\n",
        "        self.signal_received = (sig, frame)\n",
        "        print('SIGINT received. Delaying KeyboardInterrupt.')\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        signal.signal(signal.SIGINT, self.old_handler)\n",
        "        if self.signal_received:\n",
        "            self.old_handler(*self.signal_received)"
      ],
      "metadata": {
        "id": "8afav49FWo2r"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Parameter class to store weights, biases and their gradients (Similar to Tensor)\n",
        "class Parameter:\n",
        "  def __init__(self, name, value):\n",
        "    self.name = name\n",
        "    self.value = value #parameter value\n",
        "    self.grad = np.zeros_like(value) #derivative\n",
        "    self.m_grad = np.zeros_like(value) #momentum for AdaGrad (memory variables for Adagrad)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"Parameter [name = \" + self.name + \", value.shape = \"+ str(self.value.shape) +\"]\"\n",
        "\n",
        "  def zero_grad(self):\n",
        "    self.grad = np.zeros_like(self.value)"
      ],
      "metadata": {
        "id": "TIBBNWMdbufN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Parameters:\n",
        "  def __init__ (self, hidden_size, input_size, output_size):\n",
        "    self.W_f = Parameter('W_f', np.random.randn(hidden_size, input_size+hidden_size)*0.01)#INPUT to hidden layer of forgot gate [input_size+hidden_size, hidden_size]\n",
        "    self.W_i = Parameter('W_i', np.random.randn(hidden_size, input_size+hidden_size)*0.01) #INPUT to hidden layer of input gate [vocal_size+hidden_size, hidden_size]\n",
        "    self.W_g = Parameter('W_g', np.random.randn(hidden_size, input_size+hidden_size)*0.01) #INPUT to hidden layer of  gate [vocal_size+hidden_size, hidden_size]\n",
        "    self.W_o = Parameter('W_o', np.random.randn(hidden_size, input_size+hidden_size)*0.01) #INPUT to hidden layer of output gate [vocal_size+hidden_size, hidden_size]\n",
        "    self.W_hy = Parameter('W_hy', np.random.randn(output_size, hidden_size)*0.01) #hidden to output [hidden_size, input_size]\n",
        "\n",
        "    self.b_f = Parameter('b_f',np.zeros((hidden_size, 1)))  #(hidden_size X 1)\n",
        "    self.b_i = Parameter('b_i',np.zeros((hidden_size, 1)))  #(hidden_size X 1)\n",
        "    self.b_g = Parameter('b_g',np.zeros((hidden_size, 1)))  #(hidden_size X 1)\n",
        "    self.b_o = Parameter('b_o',np.zeros((hidden_size, 1)))  #(hidden_size X 1)\n",
        "    self.b_hy = Parameter('b_hy',np.zeros((output_size,1))) #output layer bias [hidden_size,1]\n",
        "  \n",
        "  def zero_grad(self):\n",
        "    for p in [self.W_f, self.W_i, self.W_g, self.W_o, self.W_hy,self.b_f, self.b_i, self.b_g, self.b_o, self.b_hy]:\n",
        "      p.zero_grad()\n",
        "  \n",
        "  def clip_grad(self):\n",
        "    for g in [self.W_f, self.W_i, self.W_g, self.W_o, self.W_hy,self.b_f, self.b_i, self.b_g, self.b_o, self.b_hy]:\n",
        "      np.clip(g.grad, -1, 1, out=g.grad) # clip to mitigate exploding gradients\n",
        "  \n",
        "  def adagrad_optimizer(self, learning_rate = 1e-1):\n",
        "    for p in [self.W_f, self.W_i, self.W_g, self.W_o, self.W_hy,self.b_f, self.b_i, self.b_g, self.b_o, self.b_hy]:\n",
        "      p.m_grad += p.grad * p.grad\n",
        "      p.value += -learning_rate * p.grad / np.sqrt(p.m_grad + 1e-8) # adagrad update\n",
        "      "
      ],
      "metadata": {
        "id": "q4WNzqAd8vDE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Class to store all the weights and biases\n",
        "class LSTM:\n",
        "  def __init__(self, hidden_size, input_size, sequence_length, learning_rate = 1e-1, weight_sd = 0.1, lstm_layers = 1):\n",
        "    self.hidden_size = hidden_size\n",
        "    self.input_size = input_size\n",
        "    self.sequence_length = sequence_length;\n",
        "    self.learning_rate = learning_rate;\n",
        "    self.weight_sd = weight_sd;\n",
        "    self.lstm_layers = lstm_layers;\n",
        "    self.lstm_parameters = []\n",
        "    self.lstm_parameters.append(LSTM_Parameters(hidden_size, input_size, input_size))\n",
        "\n",
        "    for i in range(1,lstm_layers):\n",
        "      print(i)\n",
        "      self.lstm_parameters.append(LSTM_Parameters(hidden_size, hidden_size, input_size))\n",
        "\n",
        "    self.plot_iter = np.zeros((0))\n",
        "    self.plot_loss = np.zeros((0))\n",
        "    self.smt_loss = np.zeros((0))\n",
        "\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"LSTM: [input_size = \"+str(self.input_size) +\", hidden_size = \"+ str(self.hidden_size)+\", sequence_length = \"+str(self.sequence_length)+\", lstm_layers = \"+str(self.lstm_layers)+\", learning_rate = \"+str(self.learning_rate)+\"]\"\n",
        "  \n",
        "  def zero_grads(self):\n",
        "    for i in range(self.lstm_layers):\n",
        "      self.lstm_parameters[i].zero_grad()\n",
        "  \n",
        "  def clip_grads(self):\n",
        "    for i in range(self.lstm_layers):\n",
        "      self.lstm_parameters[i].clip_grad()\n",
        "\n",
        "  def adagrad_optimizer(self):\n",
        "    for i in range(self.lstm_layers):\n",
        "      self.lstm_parameters[i].adagrad_optimizer()\n",
        "\n",
        "  # Sigmoid Function:\n",
        "  def sigmoid(self,x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "  \n",
        "  def text_generation_LSTM_cell_forward(self, input, h_prev, c_prev, layer):\n",
        "    # Stack h and x together\n",
        "    inp = np.vstack((h_prev, input)) # h before x\n",
        "\n",
        "    # Forward Pass through four gates of LSTM\n",
        "    f = self.sigmoid(np.dot(self.lstm_parameters[layer].W_f.value, inp) + self.lstm_parameters[layer].b_f.value)\n",
        "    i = self.sigmoid(np.dot(self.lstm_parameters[layer].W_i.value, inp) + self.lstm_parameters[layer].b_i.value)\n",
        "    g = np.tanh(np.dot(self.lstm_parameters[layer].W_g.value, inp) + self.lstm_parameters[layer].b_g.value)\n",
        "    o = self.sigmoid(np.dot(self.lstm_parameters[layer].W_o.value, inp)+self.lstm_parameters[layer].b_o.value)\n",
        "\n",
        "    # Calulate cell state: (Element Wise multiplication and addition)\n",
        "    c_t = np.multiply(c_prev, f) + np.multiply(i, g)\n",
        "    c_tanh = np.tanh(c_t)\n",
        "    h_t = np.multiply(c_tanh,o)   #ht generated for current timestemp (hidden_size X 1)\n",
        "\n",
        "    return h_t, c_t, f, i, g, o\n",
        "\n",
        "  def text_generation_LSTM_cell_calculate_y(self, h_t, layer):\n",
        "    # Calculate y \n",
        "    y = np.dot(self.lstm_parameters[layer].W_hy.value, h_t) + self.lstm_parameters[layer].b_hy.value #y before passing through softmax function (input_size X 1)\n",
        "    \n",
        "    # softmax function:\n",
        "    p = np.exp(y) / np.sum(np.exp(y)) # (input_size X 1)\n",
        "    return y, p\n",
        "\n",
        "  def text_generation_modified_forward(self, input, h_prev, c_prev):\n",
        "    h_t, c_t, f_t, i_t, g_t, o_t = np.empty(self.lstm_layers, dtype=object), np.empty(self.lstm_layers, dtype=object), np.empty(self.lstm_layers, dtype=object), np.empty(self.lstm_layers, dtype=object), np.empty(self.lstm_layers, dtype=object),np.empty(self.lstm_layers, dtype=object)\n",
        "    \n",
        "    for i in range(self.lstm_layers):\n",
        "      h_t[i], c_t[i], f_t[i], i_t[i], g_t[i], o_t[i] = self.text_generation_LSTM_cell_forward(input, h_prev[i], c_prev[i], i)\n",
        "      input = h_t[i]\n",
        "    \n",
        "    y, p = self.text_generation_LSTM_cell_calculate_y(h_t[self.lstm_layers-1], self.lstm_layers-1)\n",
        "\n",
        "    return h_t, c_t, f_t, i_t, g_t, o_t, y, p\n",
        "\n",
        "\n",
        "  def forward(self, input, h_prev, c_prev):\n",
        "    # Stack h and x together\n",
        "    inp = np.vstack((h_prev, input)) # h before x\n",
        "\n",
        "    # Forward Pass through four gates of LSTM\n",
        "    f = self.sigmoid(np.dot(self.lstm_parameters.W_f.value, inp) + self.lstm_parameters.b_f.value)\n",
        "    i = self.sigmoid(np.dot(self.lstm_parameters.W_i.value, inp) + self.lstm_parameters.b_i.value)\n",
        "    g = np.tanh(np.dot(self.lstm_parameters.W_g.value, inp) + self.lstm_parameters.b_g.value)\n",
        "    o = self.sigmoid(np.dot(self.lstm_parameters.W_o.value, inp)+self.lstm_parameters.b_o.value)\n",
        "\n",
        "    # Calulate cell state: (Element Wise multiplication and addition)\n",
        "    c_t = np.multiply(c_prev, f) + np.multiply(i, g)\n",
        "    c_tanh = np.tanh(c_t)\n",
        "    h_t = np.multiply(c_tanh,o)   #ht generated for current timestemp (hidden_size X 1)\n",
        "    # Calculate y \n",
        "    y = np.dot(self.lstm_parameters.W_hy.value, h_t) + self.lstm_parameters.b_hy.value #y before passing through softmax function (input_size X 1)\n",
        "    # softmax function:\n",
        "    p = np.exp(y) / np.sum(np.exp(y)) # (input_size X 1)\n",
        "    return h_t, c_t, y, p, f, i, g, o\n",
        "\n",
        "  # Multiple Layer\n",
        "  def text_generation_modified_sample_LSTM(self, h_prev, c_prev, seed_ix, sequence_length):\n",
        "    # Prepare initial input for LSTM\n",
        "    x = np.zeros((self.input_size,1))\n",
        "    x[seed_ix] = 1\n",
        "\n",
        "    h_t = h_prev\n",
        "    c_t = c_prev\n",
        "\n",
        "    ixes = []\n",
        "    for t in range(sequence_length):\n",
        "      h_t, c_t, _, _, _, _, _, p = self.text_generation_modified_forward(x, h_t, c_t)\n",
        "\n",
        "      #get index of maximum probability\n",
        "      ix = np.random.choice(range(self.input_size), p=p.ravel())\n",
        "      \n",
        "      x = np.zeros((self.input_size, 1))\n",
        "      x[ix] = 1\n",
        "      ixes.append(ix)\n",
        "\n",
        "    return ixes\n",
        "\n",
        "  # Single Layer\n",
        "  def sample_LSTM(self, h_prev, c_prev, seed_ix, sequence_length):\n",
        "    # Prepare initial input for LSTM\n",
        "    x = np.zeros((self.input_size,1))\n",
        "    x[seed_ix] = 1\n",
        "\n",
        "    h_t = h_prev\n",
        "    c_t = c_prev\n",
        "\n",
        "    ixes = []\n",
        "    for t in range(sequence_length):\n",
        "      h_t, c_t, _, p, _, _, _, _ = self.forward(x, h_t, c_t)\n",
        "\n",
        "      #get index of maximum probability\n",
        "      ix = np.random.choice(range(self.input_size), p=p.ravel())\n",
        "      \n",
        "      x = np.zeros((self.input_size, 1))\n",
        "      x[ix] = 1\n",
        "      ixes.append(ix)\n",
        "\n",
        "    return ixes\n",
        "\n",
        "  def LSTM_cell_backpropogation(self,cs_prev, hs_prev, xs, o_s, f_s, g_s, i_s, cs, dh_next, dc_next, layer):\n",
        "    dc_tanh = dh_next * o_s # dL/dc_tanh\n",
        "    dct = dc_tanh*(1-(np.tanh(cs)**2)) + dc_next # dL/dct -->dct\n",
        "\n",
        "    dcnext = dct*f_s #dL/dc_(t-1)\n",
        "\n",
        "    inp = np.vstack((hs_prev, xs)) # h before x\n",
        "\n",
        "    #Output Gate\n",
        "    d_ot = dh_next*np.tanh(cs)\n",
        "    dW_o_row = d_ot * (o_s*(1-o_s))\n",
        "    self.lstm_parameters[layer].W_o.grad += np.dot(dW_o_row, inp.T)\n",
        "    self.lstm_parameters[layer].b_o.grad += dW_o_row\n",
        "\n",
        "    #Forget Gate\n",
        "    d_ft = dct*cs_prev\n",
        "    dW_f_row = d_ft * (f_s*(1-f_s))\n",
        "    self.lstm_parameters[layer].W_f.grad += np.dot(dW_f_row,inp.T)\n",
        "    self.lstm_parameters[layer].b_f.grad += dW_f_row\n",
        "\n",
        "    #Input Gate\n",
        "    d_it = dct*g_s\n",
        "    dW_i_row = d_it * (i_s*(1-i_s))\n",
        "    self.lstm_parameters[layer].W_i.grad += np.dot(dW_i_row,inp.T)\n",
        "    self.lstm_parameters[layer].b_i.grad += dW_i_row\n",
        "\n",
        "    #Activation Gate\n",
        "    d_gt = dct*i_s\n",
        "    dW_g_row = d_gt * (1-(g_s**2))\n",
        "    self.lstm_parameters[layer].W_g.grad += np.dot(dW_g_row,inp.T)\n",
        "    self.lstm_parameters[layer].b_g.grad += dW_g_row\n",
        "    \n",
        "    dhnext = np.dot(self.lstm_parameters[layer].W_o.value[:, 0:hidden_size].T, dW_o_row) + np.dot(self.lstm_parameters[layer].W_f.value[:, 0:hidden_size].T, dW_f_row) + np.dot(self.lstm_parameters[layer].W_i.value[:, 0:hidden_size].T, dW_i_row) + np.dot(self.lstm_parameters[layer].W_g.value[:, 0:hidden_size].T, dW_g_row)\n",
        "\n",
        "    dxnext = np.dot(self.lstm_parameters[layer].W_o.value[:, hidden_size:].T, dW_o_row) + np.dot(self.lstm_parameters[layer].W_f.value[:, hidden_size:].T, dW_f_row) + np.dot(self.lstm_parameters[layer].W_i.value[:, hidden_size:].T, dW_i_row) + np.dot(self.lstm_parameters[layer].W_g.value[:, hidden_size:].T, dW_g_row)\n",
        "\n",
        "    return dhnext, dcnext, dxnext\n",
        "\n",
        "  def text_generation_modified_lossFun_LSTM(self, inputs, targets, hprev, cprev):\n",
        "    # Dictonary to store inputs, outputs, and probabilities on each state\n",
        "    xs, ys, ps = {}, {}, {}\n",
        "\n",
        "    #Dictonary of Array of Arrays to store hidden states, cell states, f, i,g,and o for each layer and state\n",
        "    hs, cs, f_s, i_s, g_s, o_s = {}, {}, {}, {}, {}, {}\n",
        "\n",
        "    #store the hidden state and cell state at time stemp t-1\n",
        "    hs[-1] = np.copy(hprev)\n",
        "    cs[-1] = np.copy(cprev)\n",
        "\n",
        "    # Initialize loss\n",
        "    loss = 0\n",
        "\n",
        "    # forward pass\n",
        "    for t in range(len(targets)):\n",
        "\n",
        "      xs[t] = np.zeros((self.input_size,1)) # encode in 1-of-k representation\n",
        "      xs[t][inputs[t]] = 1\n",
        "\n",
        "      # Forward pass:\n",
        "      hs[t], cs[t], f_s[t], i_s[t], g_s[t], o_s[t], ys[t], ps[t] = self.text_generation_modified_forward(xs[t], hs[t-1], cs[t-1])\n",
        "\n",
        "      #Cross Entropy Loss\n",
        "      loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
        "\n",
        "    self.zero_grads()\n",
        "\n",
        "    dhnext = np.empty(self.lstm_layers, dtype=object)\n",
        "    dcnext = np.empty(self.lstm_layers, dtype=object)\n",
        "    for i in range(self.lstm_layers):\n",
        "      dhnext[i] = np.zeros((self.hidden_size,1)) # reset LSTM memory\n",
        "      dcnext[i] = np.zeros((self.hidden_size,1)) # reset LSTM cell memory\n",
        "\n",
        "\n",
        "    #Backward loop: t = len(targets) to 0\n",
        "    for t in reversed(range(len(targets))):      \n",
        "      #Calculate Loss between actual and predicted\n",
        "      dy = np.copy(ps[t])\n",
        "      dy[targets[t]] -= 1\n",
        "      \n",
        "      # ht to y\n",
        "      self.lstm_parameters[self.lstm_layers-1].W_hy.grad += np.dot(dy, hs[t][self.lstm_layers-1].T) #dWhy += np.dot(Output_Delta, Hidden_Layer.T)\n",
        "      self.lstm_parameters[self.lstm_layers-1].b_hy.grad += dy # bias += sum(Output_Delta)\n",
        "\n",
        "      dhnext[self.lstm_layers-1] += np.dot(self.lstm_parameters[self.lstm_layers-1].W_hy.value.T, dy) # backprop into h ===> dL/dht --> dht\n",
        "\n",
        "      for layer in reversed(range(1, self.lstm_layers)):\n",
        "        dhnext[layer], dcnext[layer], dxnext = self.LSTM_cell_backpropogation(cs[t-1][layer], \n",
        "                                                                      hs[t-1][layer],\n",
        "                                                                      hs[t][layer-1], \n",
        "                                                                      o_s[t][layer],\n",
        "                                                                      f_s[t][layer], \n",
        "                                                                      g_s[t][layer], \n",
        "                                                                      i_s[t][layer],\n",
        "                                                                      cs[t][layer], \n",
        "                                                                      dhnext[layer], \n",
        "                                                                      dcnext[layer], \n",
        "                                                                      layer)\n",
        "        dhnext[layer-1] += dxnext\n",
        "      \n",
        "      dhnext[0], dcnext[0],_ = self.LSTM_cell_backpropogation(cs[t-1][0], \n",
        "                                                                      hs[t-1][0],\n",
        "                                                                      xs[t], \n",
        "                                                                      o_s[t][0],\n",
        "                                                                      f_s[t][0], \n",
        "                                                                      g_s[t][0], \n",
        "                                                                      i_s[t][0],\n",
        "                                                                      cs[t][0], \n",
        "                                                                      dhnext[0], \n",
        "                                                                      dcnext[0], \n",
        "                                                                      0)\n",
        "    self.clip_grads()\n",
        "\n",
        "    return loss, hs[len(targets)-1], cs[len(targets)-1]\n",
        "    \n",
        "  # Function to display updated output to console\n",
        "  def text_generation_update_status(self, batch, len_x, smooth_loss, epoch):\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.suptitle('Iteration  VS Loss')\n",
        "    ax1.plot(self.plot_iter, self.plot_loss)\n",
        "    ax2.plot(self.plot_iter, self.smt_loss) #Iteration VS Smooth Loss\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    print('Train Epoch: {} [{:.0f}/{:.0f} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch, len_x, 100. * batch / len_x, smooth_loss))\n",
        "\n",
        "\n",
        "  def train_text_generation(self, data, smooth_loss, epoch, char_to_idx, idx_to_char):\n",
        "    ptr = 0 #Pointer to track current location in data \n",
        "    iteration = 0\n",
        "    while True:\n",
        "      with DelayedKeyboardInterrupt():\n",
        "        # Initialize hidden and cell state for input\n",
        "        hprev = np.empty(self.lstm_layers, dtype=object)\n",
        "        cprev = np.empty(self.lstm_layers, dtype=object)\n",
        "        for i in range(self.lstm_layers):\n",
        "          hprev[i] = np.zeros((self.hidden_size,1))\n",
        "          cprev[i] = np.zeros((self.hidden_size,1))\n",
        "\n",
        "        if ptr+self.sequence_length+1 >= len(data):\n",
        "          return smooth_loss\n",
        "          \n",
        "        inputs = [char_to_idx[ch] for ch in data[ptr:ptr+self.sequence_length]]\n",
        "        targets = [char_to_idx[ch] for ch in data[ptr+1:ptr+self.sequence_length+1]]\n",
        "\n",
        "        loss, hprev, cprev = self.text_generation_modified_lossFun_LSTM(inputs, targets, hprev, cprev)\n",
        "\n",
        "        # Update overall loss function\n",
        "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "        \n",
        "\n",
        "        if iteration % 200 == 0:\n",
        "          self.plot_iter = np.append(self.plot_iter, [iteration+epoch*np.floor(len(data)/self.sequence_length)])\n",
        "          self.plot_loss = np.append(self.plot_loss, [loss])\n",
        "          self.smt_loss = np.append(self.smt_loss, [smooth_loss])\n",
        "\n",
        "          self.text_generation_update_status(iteration, np.floor(len(data)/self.sequence_length), smooth_loss, epoch)\n",
        "\n",
        "          self.test_text_generation(inputs[0], 200, idx_to_char)\n",
        "\n",
        "        # perform parameter update with Adagrad\n",
        "        self.adagrad_optimizer()\n",
        "        \n",
        "        ptr += self.sequence_length # move data pointer\n",
        "        iteration += 1\n",
        "\n",
        "    return smooth_loss\n",
        "\n",
        "  def test_text_generation(self, seed_idx, output_len, idx_to_char):\n",
        "    # Initialize hidden and cell state for input\n",
        "    hprev = np.empty(self.lstm_layers, dtype=object)\n",
        "    cprev = np.empty(self.lstm_layers, dtype=object)\n",
        "    for i in range(self.lstm_layers):\n",
        "      hprev[i] = np.zeros((self.hidden_size,1))\n",
        "      cprev[i] = np.zeros((self.hidden_size,1))\n",
        "\n",
        "    sample_ix = self.text_generation_modified_sample_LSTM(hprev, cprev, seed_idx, output_len)\n",
        "    txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
        "    print(\"txt: \", txt) #'----\\n %s \\n----' % (txt, )\n",
        "    return txt"
      ],
      "metadata": {
        "id": "bBzFREnEcrsc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_generation_data_preprocessing(data):\n",
        "  chars = list(set(data)) # all unique characters in the text file\n",
        "  data_size, vocab_size = len(data), len(chars) #size of all the content in text file and total number of unique characters\n",
        "\n",
        "  char_to_idx = {ch:i for i, ch in enumerate(chars)} #Chracter directory with index number\n",
        "  idx_to_char = {i:ch for i, ch in enumerate(chars)} #index to unique character mapping\n",
        "  \n",
        "  return char_to_idx, idx_to_char, data_size, vocab_size"
      ],
      "metadata": {
        "id": "DI44gozyY9Iq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyper-parameters\n",
        "hidden_size = 100 #size of hidden layer of neurons\n",
        "seq_length = 25 #number of steps to unroll the RNN for\n",
        "learning_rate = 1e-1\n",
        "weight_sd = 0.1 #weight_sd = 0.1 # Standard deviation of weights for initialization\n",
        "layer = 2"
      ],
      "metadata": {
        "id": "ny8-gHlIirBx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('input.txt', 'r').read()\n",
        "\n",
        "char_to_idx, idx_to_char, data_size, vocab_size = text_generation_data_preprocessing(data)\n",
        "\n",
        "print('data size: ', data_size, \"\\nvocab size: \", vocab_size)\n",
        "print(\"char to index: \", char_to_idx, \"\\nindex to char: \", idx_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8x_-ZKmXPQt",
        "outputId": "fe869ebe-5981-4a71-a7ea-48df963e105b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data size:  99993 \n",
            "vocab size:  62\n",
            "char to index:  {'v': 0, 'c': 1, 'l': 2, \"'\": 3, 'A': 4, 'P': 5, 'n': 6, 'J': 7, 'E': 8, 'x': 9, 'F': 10, 'B': 11, 'h': 12, 'W': 13, 'm': 14, 'o': 15, ';': 16, '?': 17, 'I': 18, 'b': 19, ':': 20, ',': 21, '-': 22, 'Q': 23, 'V': 24, 'd': 25, 's': 26, 'p': 27, '!': 28, 'y': 29, 'X': 30, 'z': 31, 'R': 32, 'U': 33, 'S': 34, 'u': 35, '.': 36, 'L': 37, 'M': 38, 'C': 39, 'H': 40, 'O': 41, '\\n': 42, 'N': 43, 'K': 44, ' ': 45, 'a': 46, 'G': 47, 'Z': 48, 'e': 49, 'f': 50, 'j': 51, 'k': 52, 'q': 53, 'g': 54, 'r': 55, 't': 56, 'T': 57, 'i': 58, 'Y': 59, 'w': 60, 'D': 61} \n",
            "index to char:  {0: 'v', 1: 'c', 2: 'l', 3: \"'\", 4: 'A', 5: 'P', 6: 'n', 7: 'J', 8: 'E', 9: 'x', 10: 'F', 11: 'B', 12: 'h', 13: 'W', 14: 'm', 15: 'o', 16: ';', 17: '?', 18: 'I', 19: 'b', 20: ':', 21: ',', 22: '-', 23: 'Q', 24: 'V', 25: 'd', 26: 's', 27: 'p', 28: '!', 29: 'y', 30: 'X', 31: 'z', 32: 'R', 33: 'U', 34: 'S', 35: 'u', 36: '.', 37: 'L', 38: 'M', 39: 'C', 40: 'H', 41: 'O', 42: '\\n', 43: 'N', 44: 'K', 45: ' ', 46: 'a', 47: 'G', 48: 'Z', 49: 'e', 50: 'f', 51: 'j', 52: 'k', 53: 'q', 54: 'g', 55: 'r', 56: 't', 57: 'T', 58: 'i', 59: 'Y', 60: 'w', 61: 'D'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = LSTM(hidden_size, vocab_size, seq_length, learning_rate, weight_sd, layer)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAaOxyywX1dI",
        "outputId": "355c2db0-68f6-4159-cdf5-958840be4671"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss at Iteration 0:\n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
        "print(\"Smooth Loss: \", smooth_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_brEXVSi2gW",
        "outputId": "c032b4c8-6cc1-4267-cea8-ab77386b530c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smooth Loss:  103.17835962612729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Generation model output before training \n",
        "print(\"Text Generation Before Trainning: \")\n",
        "print(\"========================================================\")\n",
        "lstm.test_text_generation(19, 500, idx_to_char)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "8KDuHbcmr2xS",
        "outputId": "5be2b7ba-003e-40d8-8ed2-c1e86f31aa5c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Generation Before Trainning: \n",
            "========================================================\n",
            "txt:  ltpgL-wdT;AthBaInqN!bqeu ;.'bijT:NQUo.JURRnXfTogqKkztzuNyzlTEOGnvqkQKCrQT.OUTPkRjx MflIXWb.pg!wiumAUjEgs;DHgwKg;:R;yTGNQhLtbSN:FTYS pgKLMxP?v'HX-Eiih:Z.FhMVGnFRQnRkszdRF\n",
            "XkdgbOr,sqYxTWvCPj;RMwMOLHfEV;GwmLabrj!ki,mZ!fibt:zK?hZCXZOyLuyXAjpMOTAINKmUaZcNpOCMktqzwMVSOpJJ.mR-LxIj:\n",
            "IWiVeVGQxQNXTFZYVldHyxbd\n",
            "giyCpeTwv!WTjjJ,RaIdHVZoZf:;UsNTpxZdOTaTqAPDLheTOMf wUM-VhzwCoqb;jJ;yhYYQEVrHyoFjvyOW;nMbyv:bUJNUGxLignpcm?YFz-hS.FJGTkrOWWgsr.wTkZoP?FwOsoF o-t:fXG'EzMUqkAKt-:dahpKTZhOyuSNFQbtutTWPVGfNDzfLHvWJI;w-Z\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"ltpgL-wdT;AthBaInqN!bqeu ;.'bijT:NQUo.JURRnXfTogqKkztzuNyzlTEOGnvqkQKCrQT.OUTPkRjx MflIXWb.pg!wiumAUjEgs;DHgwKg;:R;yTGNQhLtbSN:FTYS pgKLMxP?v'HX-Eiih:Z.FhMVGnFRQnRkszdRF\\nXkdgbOr,sqYxTWvCPj;RMwMOLHfEV;GwmLabrj!ki,mZ!fibt:zK?hZCXZOyLuyXAjpMOTAINKmUaZcNpOCMktqzwMVSOpJJ.mR-LxIj:\\nIWiVeVGQxQNXTFZYVldHyxbd\\ngiyCpeTwv!WTjjJ,RaIdHVZoZf:;UsNTpxZdOTaTqAPDLheTOMf wUM-VhzwCoqb;jJ;yhYYQEVrHyoFjvyOW;nMbyv:bUJNUGxLignpcm?YFz-hS.FJGTkrOWWgsr.wTkZoP?FwOsoF o-t:fXG'EzMUqkAKt-:dahpKTZhOyuSNFQbtutTWPVGfNDzfLHvWJI;w-Z\""
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "#Start Training:\n",
        "time0 = time.time()\n",
        "start_time = time.time()\n",
        "Total_Training_Time = 0\n",
        "for epoch in range(epochs):\n",
        "    smooth_loss = lstm.train_text_generation(data, smooth_loss, epoch, char_to_idx, idx_to_char)\n",
        "    Total_Training_Time += (time.time() - start_time)\n",
        "    lstm.test_text_generation(19, 500, idx_to_char)\n",
        "    print(\"Epoch: \",epoch,\" Training Time: \", Total_Training_Time, \" secs\")\n",
        "    start_time = time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "z65D0vmkUkj6",
        "outputId": "ce4f1f08-4965-4440-809c-7da339c515a4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 9 [3800/3999 (95%)]\tLoss: 41.229962\n",
            "txt:  nning words:\n",
            "Been you would anm\n",
            "I the disat: a chosore it\n",
            "Jigas,\n",
            "They woren encish.\n",
            "\n",
            "ANTHELLO:\n",
            "Hath a chalews miscondse atter your lipt thy husband till this, swealdm, ap well down a'll to a kond.\n",
            "Of \n",
            "txt:  usighfee of bruy me drike poor strund fee, crate,\n",
            "Whts frongm'd ald make o arooklf th,ar!\n",
            "\n",
            "RESMERNICIO:\n",
            "Was would a justman:\n",
            "Marry, it tadk my pinir:\n",
            "He's shall the 'over first for the serveds, No, it we days, some a to frature and dless of sawer; if she thee. :y reving intonn man: you hass huds's me\n",
            "shard Dlaventenot fast our atfant so go, gaves another may my.\n",
            "\n",
            "WARK ANEES:\n",
            "I'll pervy as deed cnazed to, cluse peidons fay ever they have letsly that I am sevenge:\n",
            "But Tut-o hour tellow\n",
            "That a are \n",
            "Epoch:  9  Training Time:  1103.7695529460907  secs\n"
          ]
        }
      ]
    }
  ]
}