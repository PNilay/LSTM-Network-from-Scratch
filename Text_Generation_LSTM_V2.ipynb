{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8MILotU4oSo"
      },
      "source": [
        "#One to Many - Multi Layer - LSTM Model (Text Generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqNHuQ0WbjTF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8afav49FWo2r"
      },
      "outputs": [],
      "source": [
        "#Signal Class to handle keybord interrupts\n",
        "import signal\n",
        "\n",
        "class DelayedKeyboardInterrupt(object):\n",
        "    def __enter__(self):\n",
        "        self.signal_received = False\n",
        "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
        "\n",
        "    def handler(self, sig, frame):\n",
        "        self.signal_received = (sig, frame)\n",
        "        print('SIGINT received. Delaying KeyboardInterrupt.')\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        signal.signal(signal.SIGINT, self.old_handler)\n",
        "        if self.signal_received:\n",
        "            self.old_handler(*self.signal_received)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIBBNWMdbufN"
      },
      "outputs": [],
      "source": [
        "# Create Parameter class to store weights, biases and their gradients (Similar to Tensor)\n",
        "class Parameter:\n",
        "  def __init__(self, name, value):\n",
        "    self.name = name\n",
        "    self.value = value #parameter value\n",
        "    self.grad = np.zeros_like(value) #derivative\n",
        "    self.m_grad = np.zeros_like(value) #momentum for AdaGrad (memory variables for Adagrad)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"Parameter [name = \" + self.name + \", value.shape = \"+ str(self.value.shape) +\"]\"\n",
        "\n",
        "  def zero_grad(self):\n",
        "    self.grad = np.zeros_like(self.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4WNzqAd8vDE"
      },
      "outputs": [],
      "source": [
        "class LSTM_Parameters:\n",
        "  def __init__ (self, hidden_size, input_size, output_size):\n",
        "    self.W_f = Parameter('W_f', np.random.randn(hidden_size, input_size+hidden_size)*0.01)#INPUT to hidden layer of forgot gate [input_size+hidden_size, hidden_size]\n",
        "    self.W_i = Parameter('W_i', np.random.randn(hidden_size, input_size+hidden_size)*0.01) #INPUT to hidden layer of input gate [vocal_size+hidden_size, hidden_size]\n",
        "    self.W_g = Parameter('W_g', np.random.randn(hidden_size, input_size+hidden_size)*0.01) #INPUT to hidden layer of  gate [vocal_size+hidden_size, hidden_size]\n",
        "    self.W_o = Parameter('W_o', np.random.randn(hidden_size, input_size+hidden_size)*0.01) #INPUT to hidden layer of output gate [vocal_size+hidden_size, hidden_size]\n",
        "    self.W_hy = Parameter('W_hy', np.random.randn(output_size, hidden_size)*0.01) #hidden to output [hidden_size, input_size]\n",
        "\n",
        "    self.b_f = Parameter('b_f',np.zeros((hidden_size, 1)))  #(hidden_size X 1)\n",
        "    self.b_i = Parameter('b_i',np.zeros((hidden_size, 1)))  #(hidden_size X 1)\n",
        "    self.b_g = Parameter('b_g',np.zeros((hidden_size, 1)))  #(hidden_size X 1)\n",
        "    self.b_o = Parameter('b_o',np.zeros((hidden_size, 1)))  #(hidden_size X 1)\n",
        "    self.b_hy = Parameter('b_hy',np.zeros((output_size,1))) #output layer bias [hidden_size,1]\n",
        "  \n",
        "  def zero_grad(self):\n",
        "    for p in [self.W_f, self.W_i, self.W_g, self.W_o, self.W_hy,self.b_f, self.b_i, self.b_g, self.b_o, self.b_hy]:\n",
        "      p.zero_grad()\n",
        "  \n",
        "  def clip_grad(self):\n",
        "    for g in [self.W_f, self.W_i, self.W_g, self.W_o, self.W_hy,self.b_f, self.b_i, self.b_g, self.b_o, self.b_hy]:\n",
        "      np.clip(g.grad, -1, 1, out=g.grad) # clip to mitigate exploding gradients\n",
        "  \n",
        "  def adagrad_optimizer(self, learning_rate = 1e-1):\n",
        "    for p in [self.W_f, self.W_i, self.W_g, self.W_o, self.W_hy,self.b_f, self.b_i, self.b_g, self.b_o, self.b_hy]:\n",
        "      p.m_grad += p.grad * p.grad\n",
        "      p.value += -learning_rate * p.grad / np.sqrt(p.m_grad + 1e-8) # adagrad update\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBzFREnEcrsc"
      },
      "outputs": [],
      "source": [
        "# LSTM Class to store all the weights and biases\n",
        "class LSTM:\n",
        "  def __init__(self, hidden_size, input_size, sequence_length, learning_rate = 1e-1, weight_sd = 0.1, lstm_layers = 1):\n",
        "    self.hidden_size = hidden_size\n",
        "    self.input_size = input_size\n",
        "    self.sequence_length = sequence_length\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_sd = weight_sd\n",
        "    self.lstm_layers = lstm_layers\n",
        "    self.lstm_parameters = []\n",
        "    self.lstm_parameters.append(LSTM_Parameters(hidden_size, input_size, input_size))\n",
        "\n",
        "    for i in range(1,lstm_layers):\n",
        "      print(i)\n",
        "      self.lstm_parameters.append(LSTM_Parameters(hidden_size, hidden_size, input_size))\n",
        "\n",
        "    self.plot_iter = np.zeros((0))\n",
        "    self.plot_loss = np.zeros((0))\n",
        "    self.smt_loss = np.zeros((0))\n",
        "\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"LSTM: [input_size = \"+str(self.input_size) +\", hidden_size = \"+ str(self.hidden_size)+\", sequence_length = \"+str(self.sequence_length)+\", lstm_layers = \"+str(self.lstm_layers)+\", learning_rate = \"+str(self.learning_rate)+\"]\"\n",
        "  \n",
        "  def zero_grads(self):\n",
        "    for i in range(self.lstm_layers):\n",
        "      self.lstm_parameters[i].zero_grad()\n",
        "  \n",
        "  def clip_grads(self):\n",
        "    for i in range(self.lstm_layers):\n",
        "      self.lstm_parameters[i].clip_grad()\n",
        "\n",
        "  def adagrad_optimizer(self):\n",
        "    for i in range(self.lstm_layers):\n",
        "      self.lstm_parameters[i].adagrad_optimizer()\n",
        "\n",
        "  # Sigmoid Function:\n",
        "  def sigmoid(self,x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "  \n",
        "  def text_generation_LSTM_cell_forward(self, input, h_prev, c_prev, layer):\n",
        "    # Stack h and x together\n",
        "    inp = np.vstack((h_prev, input)) # h before x\n",
        "\n",
        "    # Forward Pass through four gates of LSTM\n",
        "    f = self.sigmoid(np.dot(self.lstm_parameters[layer].W_f.value, inp) + self.lstm_parameters[layer].b_f.value)\n",
        "    i = self.sigmoid(np.dot(self.lstm_parameters[layer].W_i.value, inp) + self.lstm_parameters[layer].b_i.value)\n",
        "    g = np.tanh(np.dot(self.lstm_parameters[layer].W_g.value, inp) + self.lstm_parameters[layer].b_g.value)\n",
        "    o = self.sigmoid(np.dot(self.lstm_parameters[layer].W_o.value, inp)+self.lstm_parameters[layer].b_o.value)\n",
        "\n",
        "    # Calulate cell state: (Element Wise multiplication and addition)\n",
        "    c_t = np.multiply(c_prev, f) + np.multiply(i, g)\n",
        "    c_tanh = np.tanh(c_t)\n",
        "    h_t = np.multiply(c_tanh,o)   #ht generated for current timestemp (hidden_size X 1)\n",
        "\n",
        "    return h_t, c_t, f, i, g, o\n",
        "\n",
        "  def text_generation_LSTM_cell_calculate_y(self, h_t, layer):\n",
        "    # Calculate y \n",
        "    y = np.dot(self.lstm_parameters[layer].W_hy.value, h_t) + self.lstm_parameters[layer].b_hy.value #y before passing through softmax function (input_size X 1)\n",
        "    \n",
        "    # softmax function:\n",
        "    p = np.exp(y) / np.sum(np.exp(y)) # (input_size X 1)\n",
        "    return y, p\n",
        "\n",
        "  def text_generation_modified_forward(self, input, h_prev, c_prev):\n",
        "    h_t, c_t, f_t, i_t, g_t, o_t = np.empty(self.lstm_layers, dtype=object), np.empty(self.lstm_layers, dtype=object), np.empty(self.lstm_layers, dtype=object), np.empty(self.lstm_layers, dtype=object), np.empty(self.lstm_layers, dtype=object),np.empty(self.lstm_layers, dtype=object)\n",
        "    \n",
        "    for i in range(self.lstm_layers):\n",
        "      h_t[i], c_t[i], f_t[i], i_t[i], g_t[i], o_t[i] = self.text_generation_LSTM_cell_forward(input, h_prev[i], c_prev[i], i)\n",
        "      input = h_t[i]\n",
        "    \n",
        "    y, p = self.text_generation_LSTM_cell_calculate_y(h_t[self.lstm_layers-1], self.lstm_layers-1)\n",
        "\n",
        "    return h_t, c_t, f_t, i_t, g_t, o_t, y, p\n",
        "\n",
        "\n",
        "  def forward(self, input, h_prev, c_prev):\n",
        "    # Stack h and x together\n",
        "    inp = np.vstack((h_prev, input)) # h before x\n",
        "\n",
        "    # Forward Pass through four gates of LSTM\n",
        "    f = self.sigmoid(np.dot(self.lstm_parameters.W_f.value, inp) + self.lstm_parameters.b_f.value)\n",
        "    i = self.sigmoid(np.dot(self.lstm_parameters.W_i.value, inp) + self.lstm_parameters.b_i.value)\n",
        "    g = np.tanh(np.dot(self.lstm_parameters.W_g.value, inp) + self.lstm_parameters.b_g.value)\n",
        "    o = self.sigmoid(np.dot(self.lstm_parameters.W_o.value, inp)+self.lstm_parameters.b_o.value)\n",
        "\n",
        "    # Calulate cell state: (Element Wise multiplication and addition)\n",
        "    c_t = np.multiply(c_prev, f) + np.multiply(i, g)\n",
        "    c_tanh = np.tanh(c_t)\n",
        "    h_t = np.multiply(c_tanh,o)   #ht generated for current timestemp (hidden_size X 1)\n",
        "    # Calculate y \n",
        "    y = np.dot(self.lstm_parameters.W_hy.value, h_t) + self.lstm_parameters.b_hy.value #y before passing through softmax function (input_size X 1)\n",
        "    # softmax function:\n",
        "    p = np.exp(y) / np.sum(np.exp(y)) # (input_size X 1)\n",
        "    return h_t, c_t, y, p, f, i, g, o\n",
        "\n",
        "  # Multiple Layer\n",
        "  def text_generation_modified_sample_LSTM(self, h_prev, c_prev, seed_ix, sequence_length):\n",
        "    # Prepare initial input for LSTM\n",
        "    x = np.zeros((self.input_size,1))\n",
        "    x[seed_ix] = 1\n",
        "\n",
        "    h_t = h_prev\n",
        "    c_t = c_prev\n",
        "\n",
        "    ixes = []\n",
        "    for t in range(sequence_length):\n",
        "      h_t, c_t, _, _, _, _, _, p = self.text_generation_modified_forward(x, h_t, c_t)\n",
        "\n",
        "      #get index of maximum probability\n",
        "      ix = np.random.choice(range(self.input_size), p=p.ravel())\n",
        "      \n",
        "      x = np.zeros((self.input_size, 1))\n",
        "      x[ix] = 1\n",
        "      ixes.append(ix)\n",
        "\n",
        "    return ixes\n",
        "\n",
        "  # Single Layer\n",
        "  def sample_LSTM(self, h_prev, c_prev, seed_ix, sequence_length):\n",
        "    # Prepare initial input for LSTM\n",
        "    x = np.zeros((self.input_size,1))\n",
        "    x[seed_ix] = 1\n",
        "\n",
        "    h_t = h_prev\n",
        "    c_t = c_prev\n",
        "\n",
        "    ixes = []\n",
        "    for t in range(sequence_length):\n",
        "      h_t, c_t, _, p, _, _, _, _ = self.forward(x, h_t, c_t)\n",
        "\n",
        "      #get index of maximum probability\n",
        "      ix = np.random.choice(range(self.input_size), p=p.ravel())\n",
        "      \n",
        "      x = np.zeros((self.input_size, 1))\n",
        "      x[ix] = 1\n",
        "      ixes.append(ix)\n",
        "\n",
        "    return ixes\n",
        "\n",
        "  def LSTM_cell_backpropogation(self,cs_prev, hs_prev, xs, o_s, f_s, g_s, i_s, cs, dh_next, dc_next, layer):\n",
        "    # dht ==> dh_next\n",
        "    # o_s[t] ==> o_s at timestemp t\n",
        "    # f_s[t] ==> f_s\n",
        "    # g_s[t] ==> g_s\n",
        "    # i_s[t] ==> i_s\n",
        "    # xs[t] ==> xs\n",
        "    # cs[t] ==> cs ==> cell state output at t\n",
        "    # cs[t-1] ==> cs_prev\n",
        "    # hs[t-1] ==> hs_prev\n",
        "    # dcnext ==> dc_next\n",
        "    dc_tanh = dh_next * o_s # dL/dc_tanh\n",
        "    dct = dc_tanh*(1-(np.tanh(cs)**2)) + dc_next # dL/dct -->dct\n",
        "\n",
        "    dcnext = dct*f_s #dL/dc_(t-1)\n",
        "\n",
        "    inp = np.vstack((hs_prev, xs)) # h before x\n",
        "\n",
        "    #Output Gate\n",
        "    d_ot = dh_next*np.tanh(cs)\n",
        "    dW_o_row = d_ot * (o_s*(1-o_s))\n",
        "    self.lstm_parameters[layer].W_o.grad += np.dot(dW_o_row, inp.T)\n",
        "    self.lstm_parameters[layer].b_o.grad += dW_o_row\n",
        "\n",
        "    #Forget Gate\n",
        "    d_ft = dct*cs_prev\n",
        "    dW_f_row = d_ft * (f_s*(1-f_s))\n",
        "    self.lstm_parameters[layer].W_f.grad += np.dot(dW_f_row,inp.T)\n",
        "    self.lstm_parameters[layer].b_f.grad += dW_f_row\n",
        "\n",
        "    #Input Gate\n",
        "    d_it = dct*g_s\n",
        "    dW_i_row = d_it * (i_s*(1-i_s))\n",
        "    self.lstm_parameters[layer].W_i.grad += np.dot(dW_i_row,inp.T)\n",
        "    self.lstm_parameters[layer].b_i.grad += dW_i_row\n",
        "\n",
        "    #Activation Gate\n",
        "    d_gt = dct*i_s\n",
        "    dW_g_row = d_gt * (1-(g_s**2))\n",
        "    self.lstm_parameters[layer].W_g.grad += np.dot(dW_g_row,inp.T)\n",
        "    self.lstm_parameters[layer].b_g.grad += dW_g_row\n",
        "    \n",
        "    dhnext = np.dot(self.lstm_parameters[layer].W_o.value[:, 0:hidden_size].T, dW_o_row) + np.dot(self.lstm_parameters[layer].W_f.value[:, 0:hidden_size].T, dW_f_row) + np.dot(self.lstm_parameters[layer].W_i.value[:, 0:hidden_size].T, dW_i_row) + np.dot(self.lstm_parameters[layer].W_g.value[:, 0:hidden_size].T, dW_g_row)\n",
        "\n",
        "    dxnext = np.dot(self.lstm_parameters[layer].W_o.value[:, hidden_size:].T, dW_o_row) + np.dot(self.lstm_parameters[layer].W_f.value[:, hidden_size:].T, dW_f_row) + np.dot(self.lstm_parameters[layer].W_i.value[:, hidden_size:].T, dW_i_row) + np.dot(self.lstm_parameters[layer].W_g.value[:, hidden_size:].T, dW_g_row)\n",
        "\n",
        "    return dhnext, dcnext, dxnext\n",
        "\n",
        "  def text_generation_modified_lossFun_LSTM(self, inputs, targets, hprev, cprev):\n",
        "    # inputs, target ==> len(input) == len(target) == sequence_length\n",
        "    # hprev, cprev ==> array of arrays \n",
        "\n",
        "    # Dictonary to store inputs, outputs, and probabilities on each state\n",
        "    xs, ys, ps = {}, {}, {}\n",
        "\n",
        "    #Dictonary of Array of Arrays to store hidden states, cell states, f, i,g,and o for each layer and state\n",
        "    hs, cs, f_s, i_s, g_s, o_s = {}, {}, {}, {}, {}, {}\n",
        "\n",
        "    #store the hidden state and cell state at time stemp t-1\n",
        "    hs[-1] = np.copy(hprev)\n",
        "    cs[-1] = np.copy(cprev)\n",
        "\n",
        "    # Initialize loss\n",
        "    loss = 0\n",
        "\n",
        "    # forward pass\n",
        "    for t in range(len(targets)):\n",
        "\n",
        "      xs[t] = np.zeros((self.input_size,1)) # encode in 1-of-k representation\n",
        "      xs[t][inputs[t]] = 1\n",
        "\n",
        "      # Forward pass:\n",
        "      hs[t], cs[t], f_s[t], i_s[t], g_s[t], o_s[t], ys[t], ps[t] = self.text_generation_modified_forward(xs[t], hs[t-1], cs[t-1])\n",
        "\n",
        "      #Cross Entropy Loss\n",
        "      loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
        "\n",
        "    self.zero_grads()\n",
        "\n",
        "    dhnext = np.empty(self.lstm_layers, dtype=object)\n",
        "    dcnext = np.empty(self.lstm_layers, dtype=object)\n",
        "    for i in range(self.lstm_layers):\n",
        "      dhnext[i] = np.zeros((self.hidden_size,1)) # reset LSTM memory\n",
        "      dcnext[i] = np.zeros((self.hidden_size,1)) # reset LSTM cell memory\n",
        "\n",
        "\n",
        "    #Backward loop: t = len(targets) to 0\n",
        "    for t in reversed(range(len(targets))):      \n",
        "      #Calculate Loss between actual and predicted\n",
        "      dy = np.copy(ps[t])\n",
        "      dy[targets[t]] -= 1\n",
        "      \n",
        "      # ht to y\n",
        "      self.lstm_parameters[self.lstm_layers-1].W_hy.grad += np.dot(dy, hs[t][self.lstm_layers-1].T) #dWhy += np.dot(Output_Delta, Hidden_Layer.T)\n",
        "      self.lstm_parameters[self.lstm_layers-1].b_hy.grad += dy # bias += sum(Output_Delta)\n",
        "\n",
        "      dhnext[self.lstm_layers-1] += np.dot(self.lstm_parameters[self.lstm_layers-1].W_hy.value.T, dy) # backprop into h ===> dL/dht --> dht\n",
        "\n",
        "      for layer in reversed(range(1, self.lstm_layers)):\n",
        "        dhnext[layer], dcnext[layer], dxnext = self.LSTM_cell_backpropogation(cs[t-1][layer], \n",
        "                                                                      hs[t-1][layer],\n",
        "                                                                      hs[t][layer-1], \n",
        "                                                                      o_s[t][layer],\n",
        "                                                                      f_s[t][layer], \n",
        "                                                                      g_s[t][layer], \n",
        "                                                                      i_s[t][layer],\n",
        "                                                                      cs[t][layer], \n",
        "                                                                      dhnext[layer], \n",
        "                                                                      dcnext[layer], \n",
        "                                                                      layer)\n",
        "        dhnext[layer-1] += dxnext\n",
        "      \n",
        "      dhnext[0], dcnext[0],_ = self.LSTM_cell_backpropogation(cs[t-1][0], \n",
        "                                                                      hs[t-1][0],\n",
        "                                                                      xs[t], \n",
        "                                                                      o_s[t][0],\n",
        "                                                                      f_s[t][0], \n",
        "                                                                      g_s[t][0], \n",
        "                                                                      i_s[t][0],\n",
        "                                                                      cs[t][0], \n",
        "                                                                      dhnext[0], \n",
        "                                                                      dcnext[0], \n",
        "                                                                      0)\n",
        "    self.clip_grads()\n",
        "\n",
        "    return loss, hs[len(targets)-1], cs[len(targets)-1]\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Function to display updated output to console\n",
        "  def text_generation_update_status(self, batch, len_x, smooth_loss, epoch):\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.suptitle('Iteration  VS Loss')\n",
        "    ax1.plot(self.plot_iter, self.plot_loss)\n",
        "    ax2.plot(self.plot_iter, self.smt_loss) #Iteration VS Smooth Loss\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    print('Train Epoch: {} [{:.0f}/{:.0f} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch, len_x, 100. * batch / len_x, smooth_loss))\n",
        "\n",
        "\n",
        "  def train_text_generation(self, data, smooth_loss, epoch, char_to_idx, idx_to_char):\n",
        "    ptr = 0 #Pointer to track current location in data \n",
        "    iteration = 0\n",
        "    while True:\n",
        "      with DelayedKeyboardInterrupt():\n",
        "        # Initialize hidden and cell state for input\n",
        "        hprev = np.empty(self.lstm_layers, dtype=object)\n",
        "        cprev = np.empty(self.lstm_layers, dtype=object)\n",
        "        for i in range(self.lstm_layers):\n",
        "          hprev[i] = np.zeros((self.hidden_size,1))\n",
        "          cprev[i] = np.zeros((self.hidden_size,1))\n",
        "\n",
        "        if ptr+self.sequence_length+1 >= len(data):\n",
        "          return smooth_loss\n",
        "          \n",
        "        inputs = [char_to_idx[ch] for ch in data[ptr:ptr+self.sequence_length]]\n",
        "        targets = [char_to_idx[ch] for ch in data[ptr+1:ptr+self.sequence_length+1]]\n",
        "\n",
        "        loss, hprev, cprev = self.text_generation_modified_lossFun_LSTM(inputs, targets, hprev, cprev)\n",
        "\n",
        "        # Update overall loss function\n",
        "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "        \n",
        "\n",
        "        if iteration % 200 == 0:\n",
        "          self.plot_iter = np.append(self.plot_iter, [iteration+epoch*np.floor(len(data)/self.sequence_length)])\n",
        "          self.plot_loss = np.append(self.plot_loss, [loss])\n",
        "          self.smt_loss = np.append(self.smt_loss, [smooth_loss])\n",
        "\n",
        "          self.text_generation_update_status(iteration, np.floor(len(data)/self.sequence_length), smooth_loss, epoch)\n",
        "\n",
        "          self.test_text_generation(inputs[0], 200, idx_to_char)\n",
        "\n",
        "        # perform parameter update with Adagrad\n",
        "        self.adagrad_optimizer()\n",
        "        \n",
        "        ptr += self.sequence_length # move data pointer\n",
        "        iteration += 1\n",
        "\n",
        "    return smooth_loss\n",
        "\n",
        "  def test_text_generation(self, seed_idx, output_len, idx_to_char):\n",
        "    # Initialize hidden and cell state for input\n",
        "    hprev = np.empty(self.lstm_layers, dtype=object)\n",
        "    cprev = np.empty(self.lstm_layers, dtype=object)\n",
        "    for i in range(self.lstm_layers):\n",
        "      hprev[i] = np.zeros((self.hidden_size,1))\n",
        "      cprev[i] = np.zeros((self.hidden_size,1))\n",
        "\n",
        "    sample_ix = self.text_generation_modified_sample_LSTM(hprev, cprev, seed_idx, output_len)\n",
        "    txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
        "    print(\"txt: \", txt) #'----\\n %s \\n----' % (txt, )\n",
        "    return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI44gozyY9Iq"
      },
      "outputs": [],
      "source": [
        "def text_generation_data_preprocessing(data):\n",
        "  chars = list(set(data)) # all unique characters in the text file\n",
        "  data_size, vocab_size = len(data), len(chars) #size of all the content in text file and total number of unique characters\n",
        "\n",
        "  char_to_idx = {ch:i for i, ch in enumerate(chars)} #Chracter directory with index number\n",
        "  idx_to_char = {i:ch for i, ch in enumerate(chars)} #index to unique character mapping\n",
        "  \n",
        "  return char_to_idx, idx_to_char, data_size, vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny8-gHlIirBx"
      },
      "outputs": [],
      "source": [
        "#hyper-parameters\n",
        "hidden_size = 100 #size of hidden layer of neurons\n",
        "seq_length = 25 #number of steps to unroll the RNN for\n",
        "learning_rate = 1e-1\n",
        "weight_sd = 0.1 #weight_sd = 0.1 # Standard deviation of weights for initialization\n",
        "layer = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8x_-ZKmXPQt",
        "outputId": "47cc7da3-36e5-45d2-e756-4e78661ac1c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data size:  99993 \n",
            "vocab size:  62\n",
            "char to index:  {'-': 0, 'q': 1, 'd': 2, 'M': 3, 't': 4, '\\n': 5, 'b': 6, 'a': 7, 'k': 8, 'R': 9, '!': 10, 'L': 11, 'y': 12, '.': 13, 'X': 14, 'Z': 15, 'V': 16, 'S': 17, 'x': 18, 'v': 19, 'r': 20, 'u': 21, 's': 22, 'A': 23, ' ': 24, 'z': 25, 'O': 26, 'g': 27, 'c': 28, 'W': 29, 'Q': 30, 'H': 31, '?': 32, 'e': 33, 'C': 34, 'n': 35, ';': 36, 'J': 37, 'U': 38, 'l': 39, ',': 40, 'N': 41, 'o': 42, ':': 43, 'p': 44, 'h': 45, 'B': 46, 'D': 47, 'P': 48, 'T': 49, 'm': 50, 'i': 51, \"'\": 52, 'F': 53, 'I': 54, 'E': 55, 'K': 56, 'j': 57, 'Y': 58, 'f': 59, 'w': 60, 'G': 61} \n",
            "index to char:  {0: '-', 1: 'q', 2: 'd', 3: 'M', 4: 't', 5: '\\n', 6: 'b', 7: 'a', 8: 'k', 9: 'R', 10: '!', 11: 'L', 12: 'y', 13: '.', 14: 'X', 15: 'Z', 16: 'V', 17: 'S', 18: 'x', 19: 'v', 20: 'r', 21: 'u', 22: 's', 23: 'A', 24: ' ', 25: 'z', 26: 'O', 27: 'g', 28: 'c', 29: 'W', 30: 'Q', 31: 'H', 32: '?', 33: 'e', 34: 'C', 35: 'n', 36: ';', 37: 'J', 38: 'U', 39: 'l', 40: ',', 41: 'N', 42: 'o', 43: ':', 44: 'p', 45: 'h', 46: 'B', 47: 'D', 48: 'P', 49: 'T', 50: 'm', 51: 'i', 52: \"'\", 53: 'F', 54: 'I', 55: 'E', 56: 'K', 57: 'j', 58: 'Y', 59: 'f', 60: 'w', 61: 'G'}\n"
          ]
        }
      ],
      "source": [
        "data = open('input.txt', 'r').read()\n",
        "# data = \"Hello my name is nilay patel. what is your name? where are from? good to meet you\"\n",
        "\n",
        "char_to_idx, idx_to_char, data_size, vocab_size = text_generation_data_preprocessing(data)\n",
        "\n",
        "print('data size: ', data_size, \"\\nvocab size: \", vocab_size)\n",
        "print(\"char to index: \", char_to_idx, \"\\nindex to char: \", idx_to_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAaOxyywX1dI",
        "outputId": "bb9e0616-bd9c-4dec-d6f7-9a67ad88f0e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "lstm = LSTM(hidden_size, vocab_size, seq_length, learning_rate, weight_sd, layer)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_brEXVSi2gW",
        "outputId": "c5c8d3ae-debf-4754-a294-44a2caac6e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Smooth Loss:  103.17835962612729\n",
            "p:  0  seq_length:  25 len(data) 99993 n:  0\n",
            "Iteration \t Seq start\n"
          ]
        }
      ],
      "source": [
        "n, p = 0, 0\n",
        "\n",
        "#Loss at Iteration 0:\n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
        "print(\"Smooth Loss: \", smooth_loss)\n",
        "print(\"p: \", p, \" seq_length: \", seq_length, \"len(data)\", len(data), \"n: \",n)\n",
        "print(\"Iteration \\t Seq start\")\n",
        "lossData = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "z65D0vmkUkj6",
        "outputId": "6de016ec-7e95-492a-c908-fbd84840c52b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 9 [3800/3999 (95%)]\tLoss: 39.128697\n",
            "txt:  rrect and sig,rs:\n",
            "I'll dunm, hand headn; there with I parry and I\n",
            "love prone than Sir John waged bosso did font to not his writt\n",
            "This blind my fudscain fortunced, or thou has,\n",
            "That nect strain, and th\n",
            "txt:  e in say\n",
            "But enfecion upon enfence Geving\n",
            "have your came your worst\n",
            "The servif earled we with her bay: he dover. Whose wear eat must beling rastle your king\n",
            "To long be,\n",
            "Now to gimt is known in brought contriceests consent our fairty, I'll she would and blind,\n",
            "my flower, dirur tead follow Of umone:\n",
            "Thoir sope'd know Tales bed.\n",
            "A faig, I have will I\n",
            "sis, helows I knows friend; in the eaust's seat shall have his quonty of heard; from be not bonsping Lord Names in merces. These we strue mouson thy a\n",
            "Epoch:  9  Training Time:  1460.7346725463867  secs\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "#Start Training:\n",
        "time0 = time.time()\n",
        "start_time = time.time()\n",
        "Total_Training_Time = 0\n",
        "for epoch in range(epochs):\n",
        "    smooth_loss = lstm.train_text_generation(data, smooth_loss, epoch, char_to_idx, idx_to_char)\n",
        "    Total_Training_Time += (time.time() - start_time)\n",
        "    lstm.test_text_generation(19, 500, idx_to_char)\n",
        "    print(\"Epoch: \",epoch,\" Training Time: \", Total_Training_Time, \" secs\")\n",
        "    start_time = time.time()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Text_Generation_LSTM_V2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
